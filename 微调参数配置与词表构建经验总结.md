# 📘 大语言模型训练/微调参数配置与词表构建经验总结

---

## 📌 一、常用模型参数配置含义

| 参数名               | 说明                                       |
| -------------------- | ------------------------------------------ |
| `dim`                | 模型的隐藏层维度，影响整体模型容量         |
| `n_layers`           | Transformer 层数                           |
| `n_heads`            | 多头注意力机制中的头数                     |
| `n_kv_heads`         | key/value 的头数（用于 GQA，节省显存）     |
| `ffn_dim_multiplier` | FFN 的扩展倍数，如 `dim * 4`、`dim * 2.67` |
| `rope_theta`         | 相对位置编码的角度基准，影响最大上下文长度 |

---

## 📊 二、主流开源模型的参数配置（参考）

| 模型       | dim   | n_layers | n_heads | n_kv_heads | ffn_dim_multiplier | rope_theta |
| ---------- | ----- | -------- | ------- | ---------- | ------------------ | ---------- |
| LLaMA-7B   | 4096  | 32       | 32      | 32         | ≈2.7x              | 10,000     |
| GPT-2      | 768   | 12       | 12      | 12         | 4x                 | 1,000      |
| GPT-3      | 12288 | 96       | 96      | 96         | 4x                 | 1,000      |
| BLOOM-7B   | 4096  | 30       | 32      | 32         | 4x                 | 1,000      |
| Mistral-7B | 4096  | 32       | 32      | 8          | ≈2.7x              | 1e5        |

---

## 🧠 三、常用参数取值的来源

### ✅ 1. 论文与开源模型

- Meta LLaMA、GPT 系列、BLOOM 等模型在论文中公布了参数。
- 参数经过大规模实验验证，是目前训练效果与计算资源的最佳平衡。

### ✅ 2. 大规模消融实验（Ablation Study）

- 比较不同配置对模型效果、训练效率的影响。

### ✅ 3. 工程部署实践

- 显存限制下的最佳配置（如 24GB GPU 上跑 7B 模型）；
- 为部署加速而使用 GQA、调整 n_kv_heads；
- 控制上下文长度时调高 `rope_theta`。

### ✅ 4. 开源社区经验

- 来自 HuggingFace、Reddit、知乎等社区的大量实战经验。

---

## 🧃 四、词表（Tokenizer / Vocab）构建方式

词表的构建影响整个模型的编码效率和训练稳定性。

### 🔹 Tokenizer 类型常见如下：

| 类型                      | 描述                           | 示例模型        |
| ------------------------- | ------------------------------ | --------------- |
| BPE（Byte Pair Encoding） | 基于频率合并的子词方法         | GPT、LLaMA 系列 |
| SentencePiece（Unigram）  | 无需分词器的训练型子词分解方式 | T5、mT5         |
| WordPiece                 | 更偏向词级别，字符分解能力较弱 | BERT 系列       |

### 🔹 构建步骤（以 BPE 为例）：

1. **准备语料**

   - 数量建议：百万句以上，数十 GB 最佳；
   - 类型：尽量贴近目标领域（如法律、医疗、多语言等）。

2. **清洗与格式化**

   - 去除乱码、空行；
   - 按行拆分句子存入 `.txt` 文件。

3. **训练 Tokenizer**

   - 使用工具如 [tokenizers](https://github.com/huggingface/tokenizers)、SentencePiece。

   - 示例命令（使用 HuggingFace `tokenizers`）：

     ```bash
     tokenizer-train        --files data.txt        --vocab-size 32000        --model-type bpe        --output vocab/
     ```

4. **生成输出**

   - `vocab.json` / `merges.txt`（BPE）；
   - 或 `.model` / `.vocab`（SentencePiece）。

5. **嵌入模型**

   - 训练前读取词表，嵌入到 `Embedding` 层；
   - vocab size 要与 `Embedding` 层维度对齐。

### 🔹 经验建议

- 通用模型建议 vocab size ≈ 32K；
- 专业领域建议适当缩减/扩展；
- 中文建议采用字符级 BPE 或 SentencePiece。

---

## 🚀 五、训练/微调时参数选择建议

| 场景                  | 建议配置                                                     |
| --------------------- | ------------------------------------------------------------ |
| **从零训练 7B 模型**  | `dim=4096`，`n_layers=32`，`n_heads=32`，`ffn=2.67x`，`n_kv_heads=8` |
| **小模型部署 (1.3B)** | `dim=2048`，`n_layers=24`，`n_heads=16`，GQA=4，`ffn=4x`     |
| **长文本场景**        | `rope_theta=1e5` 或 `1e6`，配合 FlashAttention               |
| **推理显存有限**      | 减少 `n_kv_heads`（如 GQA=8），或 `ffn=2.5x`                 |

---

## 📎 六、参数合理性反推（实际工程中常用）

- 计算总参数规模估算公式（粗略）：

  ```
  total_params ≈ 12 * n_layers * dim²
  ```

- 每层的 attention 头数建议：

  ```
  dim / n_heads ≈ 64 ~ 128
  ```

- 想让模型支持更长上下文：

  - 增加 `rope_theta`
  - 或采用 yaRN / NTK-aware RoPE 等技术

---

## 🧰 七、推荐工具与资料

- 📘 LLaMA 系列论文（Meta）
- 🔬 GPT 技术报告（OpenAI）
- 🛠️ [llama.cpp](https://github.com/ggerganov/llama.cpp)
- 📦 HuggingFace Transformers
- 🧩 [tokenizers](https://github.com/huggingface/tokenizers)
